# 006.features.01: Sight-Based Analysis Mode

**Status:** Implemented (not tested)
**Priority:** Medium
**Branch:** `feature/sight-based-analysis`

## Objective

Add a `--sight-mode` CLI flag that uses screenshots for decision-making while still using HTML selectors for executing actions.

## Context

Currently, the simulated user "sees" pages through extracted HTML properties (text, aria-label, data-testid, etc.). This works well for accessible pages but fails when:
- Icons have no text labels or aria attributes
- Visual context (colors, layout, images) is important
- The page uses unconventional patterns

Sight mode sends screenshots to a vision-capable LLM to decide what to interact with, then maps the visual selection back to HTML elements for reliable action execution.

## Technical Details

### Architecture

```
┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐
│   Screenshot    │ --> │  Vision LLM      │ --> │  Coordinate     │
│   Capture       │     │  (Claude)        │     │  Mapping        │
└─────────────────┘     └──────────────────┘     └─────────────────┘
                                                         │
                                                         v
┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐
│   Execute via   │ <-- │  Find Element    │ <-- │  Bounding Box   │
│   HTML Selector │     │  at Coords       │     │  Overlap        │
└─────────────────┘     └──────────────────┘     └─────────────────┘
```

### Files to Modify

1. **src/cli.ts** - Add `--sight-mode` flag
2. **src/lib/page-analyzer.ts** - Add screenshot capture function
3. **src/lib/llm.ts** - Add vision-based analysis function with base64 image support
4. **src/lib/session.ts** - Integrate sight mode into the main loop

### Implementation Steps

1. Add `--sight-mode` CLI flag (pass through to session options)
2. Add `captureScreenshot()` function to page-analyzer that returns base64 PNG
3. Create `getNextActionFromScreenshot()` in llm.ts that:
   - Sends screenshot to Claude with vision prompt
   - Asks LLM to describe location (e.g., "the share button in the top right")
   - Returns coordinates or element description
4. Create `findElementAtCoordinates()` to map visual selection to HTML element:
   - Use bounding boxes from page analysis
   - Find element whose bounds contain the coordinates
   - Fall back to nearest element if no exact match
5. Modify session loop to use vision analysis when `sightMode: true`

### Vision Prompt Strategy

The LLM should:
1. Receive the screenshot + goal + persona context
2. Describe what it sees and what it wants to interact with
3. Return either:
   - Coordinates (x, y) of the target
   - OR a description that can be matched to elements (e.g., "the blue button labeled Submit")

### Element Mapping

Given coordinates (x, y) from vision analysis:
```typescript
function findElementAtCoordinates(
  elements: PageElement[],
  x: number,
  y: number
): PageElement | null {
  // Find elements whose bounding box contains the point
  const hits = elements.filter(el =>
    x >= el.bounds.x &&
    x <= el.bounds.x + el.bounds.width &&
    y >= el.bounds.y &&
    y <= el.bounds.y + el.bounds.height
  );

  // Return smallest (most specific) element
  return hits.sort((a, b) =>
    (a.bounds.width * a.bounds.height) - (b.bounds.width * b.bounds.height)
  )[0] ?? null;
}
```

## Acceptance Criteria

- [ ] `--sight-mode` flag is accepted by CLI
- [ ] Screenshots are captured and sent to LLM in sight mode
- [ ] LLM returns visual decisions (coordinates or descriptions)
- [ ] Coordinates are mapped back to HTML elements
- [ ] Actions execute using HTML selectors (not coordinates)
- [ ] Regular mode still works unchanged
- [ ] Session transcripts indicate when sight mode is used

## Testing

1. Run same persona with and without `--sight-mode`
2. Test on page with icon-only buttons (no text/aria labels)
3. Verify actions execute correctly via selectors
4. Check transcript logs for sight mode indicators

```bash
# Regular mode
abra run personas/example.yaml

# Sight mode
abra run personas/example.yaml --sight-mode
```

## Notes

- Vision API calls may be slower than text-only analysis
- Screenshot capture adds overhead (~100-200ms)
- Consider caching/reusing screenshots within same page state
