# Task: Evaluate Abra vs Puppet for Automated Testing

## Overview

Evaluate whether Abra (AI-driven persona simulation) could be used as an alternative or complement to Puppet (traditional browser automation) for automated testing scenarios.

## Context

**Puppet** is a browser automation library built on Playwright with human-like cursor movements. It's designed for:
- Deterministic test scripts
- Explicit selectors and assertions
- Fast, repeatable test execution
- CI/CD integration

**Abra** is an AI-driven user simulation platform that:
- Uses personas with goals and jobs-to-be-done
- Makes autonomous decisions based on page context
- Produces video recordings of sessions
- Simulates realistic user behavior

## Evaluation Criteria

### 1. Test Reliability
| Criteria | Puppet | Abra |
|----------|--------|------|
| Deterministic results | Yes - same inputs = same outputs | No - LLM decisions vary |
| Flakiness | Low (with proper waits) | Higher (AI interpretation varies) |
| Reproducibility | High | Low |

### 2. Test Creation Effort
| Criteria | Puppet | Abra |
|----------|--------|------|
| Selector identification | Manual - requires data-testid | Automatic - AI finds elements |
| Flow definition | Explicit step-by-step | Goal-based, AI figures out steps |
| Maintenance when UI changes | Update selectors | May self-heal (AI adapts) |

### 3. Coverage & Discovery
| Criteria | Puppet | Abra |
|----------|--------|------|
| Exploratory testing | No - only tests what's scripted | Yes - may find unexpected paths |
| Edge case discovery | Only if explicitly written | May stumble upon edge cases |
| Regression testing | Excellent | Poor (non-deterministic) |

### 4. Execution Speed
| Criteria | Puppet | Abra |
|----------|--------|------|
| Per-test speed | Fast (ms-level waits) | Slow (LLM calls + thinking delays) |
| Parallelization | Easy | Limited by LLM rate limits |
| CI/CD suitability | Excellent | Poor (slow, non-deterministic) |

### 5. Debugging & Failure Analysis
| Criteria | Puppet | Abra |
|----------|--------|------|
| Failure attribution | Clear - assertion failed at line X | Fuzzy - AI couldn't achieve goal |
| Reproducibility of failures | Easy | Difficult |
| Evidence artifacts | Screenshots, traces | Video recordings, transcripts |

## Potential Use Cases for Abra in Testing

### Good Fit
1. **Exploratory testing** - Let AI personas explore and find issues humans might miss
2. **UX validation** - Verify that goals are achievable without explicit instructions
3. **Smoke testing** - Can a user accomplish basic tasks?
4. **Accessibility discovery** - How do different persona types interpret the UI?
5. **Onboarding flow validation** - Can new users complete key journeys?

### Poor Fit
1. **Regression testing** - Need deterministic, repeatable results
2. **CI/CD gates** - Non-deterministic results can't gate deployments
3. **Performance benchmarking** - Too much variance
4. **API contract testing** - No advantage over direct API calls

## Hybrid Approach

Consider using both tools for different purposes:

```
┌─────────────────────────────────────────────────────────────┐
│                     Testing Strategy                         │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  CI/CD Pipeline (every commit)                              │
│  └── Puppet: Fast, deterministic regression tests           │
│                                                             │
│  Nightly/Weekly (scheduled)                                 │
│  └── Abra: Exploratory persona simulations                  │
│      └── Review videos for UX issues                        │
│      └── Flag unexpected failures for investigation         │
│                                                             │
│  Pre-release (manual trigger)                               │
│  └── Abra: Full persona suite across user types             │
│      └── Validate all key journeys are achievable           │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

## Questions to Answer

1. **Can Abra detect regressions?**
   - If a button is removed, will the persona report failure consistently?
   - How do we distinguish "AI couldn't figure it out" from "feature is broken"?

2. **What's the cost model?**
   - LLM API costs per test run
   - Time cost (minutes vs seconds)

3. **Can we make Abra more deterministic?**
   - Lower temperature settings?
   - Constrained action spaces?
   - Caching LLM decisions for identical page states?

4. **Integration possibilities**
   - Can Abra generate Puppet test scripts from successful runs?
   - Can failed Abra sessions become bug reports automatically?

## Evaluation Results (2026-01-09)

### Test Executed: DuckDuckGo Search Task

**Goal:** "Search for 'automated testing tools' using the search box"

**Runs:** 3 consecutive executions with identical persona/goal

### Consistency Results

| Run | Duration | Actions | Status | Elements Used |
|-----|----------|---------|--------|---------------|
| 1   | 53.4s    | 2       | completed | element 2 (search), element 4 (submit) |
| 2   | 52.0s    | 2       | completed | element 2 (search), element 4 (submit) |
| 3   | 59.4s    | 2       | completed | element 2 (search), element 4 (submit) |

**Analysis:**
- **100% success rate** across all 3 runs
- **Identical action count** (2 actions each)
- **Identical element selection** (same elements clicked/typed in)
- **Duration variance:** ~7% (54.9s average, 7.4s spread)
- **Thought phrasing varied** but decisions were consistent

### LLM Call Analysis

Per the `src/lib/llm.ts` implementation:
- 1 LLM call per action cycle (analyze → decide → act)
- Simple search task: 3 LLM calls total (2 actions + 1 completion check)

**Cost Estimate (Claude Sonnet pricing):**
- Input: ~1,500 tokens/call (system prompt + page state + history)
- Output: ~75 tokens/call (JSON action response)
- Cost per call: ~$0.005
- Cost per goal: ~$0.015 (3 calls)
- Complex multi-step goal: ~$0.05-0.10 (10-20 calls)

**Time Cost:**
- Simple goal: ~55 seconds
- Equivalent Puppet test: ~2-3 seconds (deterministic clicks)
- **Abra is ~20x slower** due to LLM latency and thinking delays

### Key Findings

1. **Higher consistency than expected:** The LLM made identical element selections across all runs for a straightforward task. The variance was primarily in timing, not decisions.

2. **Action patterns are stable:** For well-defined goals, Abra chooses the same approach repeatedly (type into search → click submit).

3. **Thought diversity, action consistency:** The persona's "internal monologue" varied between runs, but the actual actions taken were identical.

4. **Speed is the main limitation:** The ~55s execution time makes Abra unsuitable for CI/CD gates, but acceptable for scheduled exploratory runs.

### Answers to Key Questions

**1. Can Abra detect regressions?**
- Likely yes for major UI changes (missing elements)
- Distinguishing "AI confusion" from "broken feature": check if goal fails consistently (3+ runs)

**2. What's the cost model?**
- ~$0.015 per simple goal (3 LLM calls)
- ~$0.05-0.10 per complex goal (10-20 LLM calls)
- Time: 1-3 minutes per goal vs 2-10 seconds with Puppet

**3. Can we make Abra more deterministic?**
- Already more consistent than expected for simple tasks
- Session persistence helps maintain context
- Caching could work for identical page states (not implemented)

**4. Integration possibilities**
- Abra → Puppet script generation is feasible:
  - Session transcript contains element IDs and actions
  - Could generate Playwright selectors from session data
- Bug report generation from failures: transcripts already provide this

## Next Steps

- [x] Run Abra against a test application with known issues - does it find them?
- [ ] Compare time-to-create for equivalent test coverage
- [x] Measure consistency: run same persona/goal 3x, analyze variance
- [ ] Prototype: Abra session → Puppet test script generator
- [x] Cost analysis: LLM calls per typical user journey

## Decision

Based on evaluation:

- [x] **Abra suitable for exploratory/UX testing alongside Puppet**
- [ ] Abra not suitable for testing purposes
- [ ] Further investigation needed

**Recommendation:** Use Abra for scheduled exploratory testing and UX validation. Keep Puppet for CI/CD regression tests. The hybrid approach from the "Testing Strategy" diagram above is validated by these results.
